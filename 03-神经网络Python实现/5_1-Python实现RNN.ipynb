{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1, dropout = 0.1):\n",
    "        super(LSTMCell, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        ih, hh = [], []\n",
    "        for i in range(num_layers):\n",
    "            if i==0:\n",
    "                ih.append(nn.Linear(input_size, 4 * hidden_size))\n",
    "                hh.append(nn.Linear(hidden_size, 4 * hidden_size))\n",
    "            else:\n",
    "                ih.append(nn.Linear(hidden_size, 4 * hidden_size))\n",
    "                hh.append(nn.Linear(hidden_size, 4 * hidden_size))\n",
    "        self.w_ih = nn.ModuleList(ih)\n",
    "        self.w_hh = nn.ModuleList(hh)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        if hidden[0].shape[0] != self.num_layers:\n",
    "            hidden = (\n",
    "                torch.tile(hidden[0], [self.num_layers,1,1]),\n",
    "                torch.tile(hidden[1], [self.num_layers,1,1]))\n",
    "\n",
    "        hy, cy = [], []\n",
    "        for i in range(self.num_layers):\n",
    "            hx, cx = hidden[0][i], hidden[1][i]\n",
    "            gates = self.w_ih[i](input) + self.w_hh[i](hx)\n",
    "            i_gate, f_gate, c_gate, o_gate = gates.chunk(4, 1)\n",
    "            i_gate = torch.sigmoid(i_gate)\n",
    "            f_gate = torch.sigmoid(f_gate)\n",
    "            c_gate = torch.tanh(c_gate)\n",
    "            o_gate = torch.sigmoid(o_gate)\n",
    "            ncx = (f_gate * cx) + (i_gate * c_gate)\n",
    "            nhx = o_gate * torch.tanh(ncx)\n",
    "            cy.append(ncx)\n",
    "            hy.append(nhx)\n",
    "            input = self.dropout(nhx)\n",
    "\n",
    "        hy, cy = torch.stack(hy, 0), torch.stack(cy, 0)  # number of layer * batch * hidden\n",
    "        return hy, cy\n",
    "\n",
    "lstm = LSTMCell(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10) # [Sequence Length, Batch Size, Input Size]\n",
    "hx = torch.randn(3, 20) # [Batch Size, Hidden Size]\n",
    "cx = torch.randn(3, 20) # [Batch Size, Cell Size]\n",
    "output = []\n",
    "for i in range(input.size()[0]):\n",
    "    hx, cx = lstm(input[i], (hx, cx))\n",
    "    output.append(hx)\n",
    "output = torch.stack(output, dim=0)\n",
    "\n",
    "#著作权归作者所有。\n",
    "#商业转载请联系作者获得授权,非商业转载请注明出处。\n",
    "#原文: https://0809zheng.github.io/2020/03/07/RNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ced85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
